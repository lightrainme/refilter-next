import type { NextApiRequest, NextApiResponse } from "next";
import OpenAI from "openai";
import fs from "fs";
import path from "path";

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const cachePath = path.join(process.cwd(), "cache", "reviews.json");

function loadCache() {
  if (!fs.existsSync(cachePath)) return {};
  try {
    return JSON.parse(fs.readFileSync(cachePath, "utf8") || "{}");
  } catch {
    return {};
  }
}

function saveCache(cache: Record<string, any>) {
  fs.mkdirSync(path.dirname(cachePath), { recursive: true });
  fs.writeFileSync(cachePath, JSON.stringify(cache, null, 2), "utf8");
}

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  const { productName, productNames } = req.body;

  const names =
    Array.isArray(productNames) && productNames.length > 0
      ? productNames
      : productName
      ? [productName]
      : [];

  if (names.length === 0) {
    return res.status(400).json({ error: "Missing product name(s)" });
  }

  const cache = loadCache();
  const uncached = names.filter((n) => !cache[n]);

  try {
    for (const name of uncached) {
      const prompt = `
ÏïÑÎûòÏùò ÏÉÅÌíà Î™©Î°ùÏùÑ Î≥¥Í≥†, Í∞Å ÏÉÅÌíàÏùò ÏÜåÎπÑÏûê Î¶¨Î∑∞Î•º Í∏∞Î∞òÏúºÎ°ú Ïû•Ï†ê 3Í∞ÄÏßÄÏôÄ Îã®Ï†ê 3Í∞ÄÏßÄÎ•º ÏòàÏ∏°Ìï¥ÏÑú ÏöîÏïΩÌï¥Ï§ò.
JSON Î∞∞Ïó¥Î°ú ÏïÑÎûò ÌòïÏãùÏúºÎ°úÎßå Ï∂úÎ†•Ìï¥Ï§ò.

[
  {
    "name": "ÏÉÅÌíàÎ™Ö",
    "pros": ["Ïû•Ï†ê1", "Ïû•Ï†ê2", "Ïû•Ï†ê3"],
    "cons": ["Îã®Ï†ê1", "Îã®Ï†ê2", "Îã®Ï†ê3"]
  }
]

ÏÉÅÌíà Î™©Î°ù:
1. ${name}
      `.trim();

      const completion = await client.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [{ role: "user", content: prompt }],
        temperature: 0.7,
      });

      let content = completion.choices[0].message?.content || "";
      content = content.replace(/```json|```/gi, "").trim();

      try {
        const parsed = JSON.parse(content);
        parsed.forEach((entry: any) => {
          if (entry?.name && entry?.pros && entry?.cons) {
            cache[entry.name] = entry;
          }
        });
      } catch (e) {
        console.error(`‚ö†Ô∏è GPT JSON ÌååÏã± Ïã§Ìå® (ÏÉÅÌíà: ${name})`, e);
      }

      // ‚úÖ Îπà Í≤∞Í≥ºÏùº Í≤ΩÏö∞ Ïû¨ÏãúÎèÑ Î°úÏßÅ
      if (!cache[name] || !cache[name].pros?.length || !cache[name].cons?.length) {
        console.log(`üîÅ ${name} Ïû¨ÏöîÏïΩ ÏãúÎèÑ Ï§ë...`);
        try {
          const retry = await client.chat.completions.create({
            model: "gpt-4o-mini",
            messages: [{ role: "user", content: prompt }],
            temperature: 0.7,
          });

          let retryContent = retry.choices[0].message?.content || "";
          retryContent = retryContent.replace(/```json|```/gi, "").trim();

          const retryParsed = JSON.parse(retryContent);
          if (Array.isArray(retryParsed) && retryParsed[0]?.pros?.length) {
            cache[name] = retryParsed[0];
            console.log(`‚úÖ ${name} Ïû¨ÏöîÏïΩ ÏÑ±Í≥µ`);
          } else {
            console.log(`‚ùå ${name} Ïû¨ÏöîÏïΩ Ïã§Ìå® (Îπà Í≤∞Í≥º)`);
          }
        } catch (retryError) {
          console.error(`‚ùå ${name} Ïû¨ÏöîÏïΩ Ï§ë Ïò§Î•ò Î∞úÏÉù`, retryError);
        }
      }

      if (!cache[name]) {
        cache[name] = { name, pros: [], cons: [] };
      }

      // ‚úÖ ÏÉÅÌíà Îã®ÏúÑÎ°ú Ï∫êÏãú Î≥ëÌï© Î∞è Ï†ÄÏû•
      const currentCache = loadCache();
      currentCache[name] = cache[name];
      saveCache(currentCache);
    }

    names.forEach((n) => {
      if (!cache[n]) cache[n] = { name: n, pros: [], cons: [] };
    });

    const results = names.map((n) => cache[n] || { pros: [], cons: [] });
    return res.status(200).json(results);
  } catch (error: any) {
    console.error("‚ùå GPT ÏöîÏïΩ Ïã§Ìå®:", error.message);
    return res.status(500).json({ error: "Failed to generate summaries" });
  }
}